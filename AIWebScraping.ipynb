{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7af255-828e-48af-9651-edd8e5b47343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0058caa-0a03-4c5c-ac66-d4e5c4ef7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = pd.DataFrame(columns=[\n",
    "    'Index', 'UsingIP', 'LongURL', 'ShortURL', 'Symbol@', 'Redirecting//', 'PrefixSuffix-', \n",
    "    'SubDomains', 'HTTPS', 'DomainRegLen', 'Favicon', 'NonStdPort', 'HTTPSDomainURL', \n",
    "    'RequestURL', 'AnchorURL', 'LinksInScriptTags', 'ServerFormHandler', 'InfoEmail', \n",
    "    'AbnormalURL', 'WebsiteForwarding', 'StatusBarCust', 'DisableRightClick', \n",
    "    'UsingPopupWindow', 'IframeRedirection', 'AgeofDomain', 'DNSRecording', 'WebsiteTraffic', \n",
    "    'PageRank', 'GoogleIndex', 'LinksPointingToPage', 'StatsReport', 'class'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8485101c-5be3-4424-89ac-e3c36185e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_using_ip(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    \n",
    "    # Check if parsed_url.hostname is not None before replacing\n",
    "    return int(parsed_url.hostname and parsed_url.hostname.replace('.', '').isdigit())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6971e5-abdc-49d0-b52a-cf30a757bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_long_url(url, threshold=20):\n",
    "    return int(len(url) > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d61534-a481-4919-af6b-11ad9e12c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_short_url(url, threshold=5):\n",
    "    return int(len(url) < threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70fda49d-b7dd-423e-bdda-26ba1d446d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_symbol_at(url):\n",
    "    return int('@' in url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee1301f-f07f-4c38-9a94-74db1dec4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_redirecting(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return int('//' in parsed_url.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84f043e1-cfe7-4248-83ef-24be39e171fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prefix_suffix(url):\n",
    "    return int('-' in urlparse(url).netloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "047b5c45-cbe4-4b5f-a583-c3fda44c67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subdomains(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    return len(extracted.subdomain.split('.')) if extracted.subdomain else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7328fab-2c90-4dbb-a064-7b5401ee825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_https(url):\n",
    "    return int(urlparse(url).scheme == 'https')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c787226a-1539-4cc6-b320-fb5acd11fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_reg_len(url):\n",
    "    return len(urlparse(url).netloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f08f16bc-d931-42cc-82a1-40bb4cd87393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_favicon(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        favicon_tag = soup.find('link', rel=['icon', 'shortcut icon', 'apple-touch-icon'])\n",
    "        if favicon_tag:\n",
    "            return 1  # Found favicon\n",
    "\n",
    "\n",
    "        default_favicon_url = urljoin(url, '/favicon.ico')\n",
    "        response = requests.head(default_favicon_url)\n",
    "        if response.status_code == 200:\n",
    "            return 1  # Found favicon\n",
    "\n",
    "\n",
    "        common_favicon_paths = [\n",
    "            '/apple-touch-icon.png',\n",
    "            '/apple-touch-icon-precomposed.png',\n",
    "            '/apple-touch-icon-120x120.png',\n",
    "            '/apple-touch-icon-152x152.png',\n",
    "        ]\n",
    "\n",
    "        for path in common_favicon_paths:\n",
    "            favicon_url = urljoin(url, path)\n",
    "            response = requests.head(favicon_url)\n",
    "            if response.status_code == 200:\n",
    "                return 1  # Found favicon\n",
    "\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return 0  # No favicon found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d742154c-46bd-4a08-a093-c740d1671ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_non_std_port(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return int(parsed_url.port not in {80, 443} and parsed_url.port is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bb5ba18-9813-49e4-8a37-0502cee4c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_https_domain_url(url):\n",
    "    return int(url.startswith('https://'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "519a4120-ee11-4047-8cd0-1ae88f07d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_request_url(url):\n",
    "    return int('request' in url.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dbc4f5e-10dd-4e6e-aeb1-17b52c8de467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_anchor_url(url):\n",
    "    return int('#' in url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15d49917-9ce7-4aab-87fe-245957ce0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_links_in_script_tags(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        script_tags = soup.find_all('script')\n",
    "        return int(any('src' in tag.attrs for tag in script_tags))\n",
    "    except Exception as e:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07d4a4a7-0b59-470b-b078-0f24cec31aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "def extract_server_form_handler(url):\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Check for the presence of form tags\n",
    "        form_tags = soup.find_all('form')\n",
    "\n",
    "        # Check for specific form attributes and input types\n",
    "        for form in form_tags:\n",
    "            action = form.get('action', '')\n",
    "            method = form.get('method', '')\n",
    "            print(f\"Form Action: {action}, Method: {method}\")\n",
    "\n",
    "            # Check for specific input types within the form\n",
    "            input_elements = form.find_all('input')\n",
    "            for input_element in input_elements:\n",
    "                input_type = input_element.get('type', '')\n",
    "                print(f\"Input Type: {input_type}\")\n",
    "\n",
    "        # Use Selenium for dynamic content (e.g., JavaScript-generated forms)\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')  # Run in headless mode (no GUI)\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Execute JavaScript to modify or generate content\n",
    "        # (Note: This is a simplified example, and real scenarios may vary)\n",
    "        driver.execute_script(\"document.getElementById('someElement').innerHTML = '<form>...</form>';\")\n",
    "\n",
    "        # Fetch the updated content\n",
    "        updated_html = driver.page_source\n",
    "\n",
    "        # Continue with BeautifulSoup analysis on the updated content\n",
    "        soup_dynamic = BeautifulSoup(updated_html, 'html.parser')\n",
    "        dynamic_form_tags = soup_dynamic.find_all('form')\n",
    "\n",
    "        # Additional analysis on dynamic content if needed\n",
    "\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "        # Return 1 if there are form tags, indicating form handling, else return 0\n",
    "        return int(len(form_tags) > 0)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions\n",
    "        print(f\"Error analyzing form handling for {url}: {str(e)}\")\n",
    "        return 0  # Placeholder value or 0 if an error occurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a94fc6a-c317-48c1-abcb-acd5ab895204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_email(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return int('@' in parsed_url.netloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c31dce1-ee89-4dd1-a30d-8a161d1d4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abnormal_url(url):\n",
    "    return int(not url.isalnum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4405147d-6199-4cbb-89bb-eed1ed4b450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_website_forwarding(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        return int(response.history)  # Non-zero history indicates redirection\n",
    "    except Exception as e:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f9b5308-3785-413a-9889-0ca35f292dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_status_bar_cust(url):\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Check for the presence of JavaScript code affecting the status bar\n",
    "        script_tags = soup.find_all('script')\n",
    "        for script_tag in script_tags:\n",
    "            if 'window.status' in script_tag.get_text():\n",
    "                return 1  # JavaScript code affecting the status bar found\n",
    "\n",
    "        # Return 0 if no relevant JavaScript code is found\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions\n",
    "        print(f\"Error analyzing StatusBarCust for {url}: {str(e)}\")\n",
    "        return 0  # Placeholder value or 0 if an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10e33441-ff02-4aa2-9c12-f718731afb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_disable_right_click(url):\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Check for the presence of JavaScript code disabling right-click\n",
    "        script_tags = soup.find_all('script')\n",
    "        for script_tag in script_tags:\n",
    "            if 'event.button==2' in script_tag.get_text():\n",
    "                return 1  # JavaScript code disabling right-click found\n",
    "\n",
    "        # Return 0 if no relevant JavaScript code is found\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions\n",
    "        print(f\"Error analyzing DisableRightClick for {url}: {str(e)}\")\n",
    "        return 0  # Placeholder value or 0 if an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1749aa8d-af13-4901-9c3c-4ea462b27f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_using_popup_window(url):\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Check for the presence of JavaScript code using popup windows\n",
    "        script_tags = soup.find_all('script')\n",
    "        for script_tag in script_tags:\n",
    "            if 'window.open(' in script_tag.get_text():\n",
    "                return 1  # JavaScript code using popup window found\n",
    "\n",
    "        # Return 0 if no relevant JavaScript code is found\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions\n",
    "        print(f\"Error analyzing UsingPopupWindow for {url}: {str(e)}\")\n",
    "        return 0  # Placeholder value or 0 if an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eaf62885-6d64-4ad4-a98d-5eb8d57fcf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_iframe_redirection(url):\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Check for the presence of iframe redirection\n",
    "        iframe_tags = soup.find_all('iframe')\n",
    "        for iframe_tag in iframe_tags:\n",
    "            if 'http-equiv=\"refresh\"' in iframe_tag.get('content', ''):\n",
    "                return 1  # Iframe redirection found\n",
    "\n",
    "        # Return 0 if no relevant iframe redirection is found\n",
    "        return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions\n",
    "        print(f\"Error analyzing IframeRedirection for {url}: {str(e)}\")\n",
    "        return 0  # Placeholder value or 0 if an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f79b2790-9993-48a1-be6f-b2bf959c7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whois\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_age_of_domain(url):\n",
    "    try:\n",
    "        # Extract the domain from the URL\n",
    "        domain = url.split('//')[-1].split('/')[0]\n",
    "\n",
    "        # Query WHOIS information\n",
    "        domain_info = whois.whois(domain)\n",
    "\n",
    "        # Extract the creation date from the WHOIS response\n",
    "        creation_dates = domain_info.creation_date\n",
    "        if not creation_dates:\n",
    "            return 0  # Unable to determine domain age\n",
    "\n",
    "        # If creation_dates is a list, use the first element\n",
    "        if isinstance(creation_dates, list):\n",
    "            creation_date = creation_dates[0]\n",
    "        else:\n",
    "            creation_date = creation_dates\n",
    "\n",
    "        # Calculate the domain age in years\n",
    "        today = datetime.now()\n",
    "        age_of_domain = (today - creation_date).days // 365\n",
    "\n",
    "        # Return 1 if the age is greater than 1 year, else return 0\n",
    "        return int(age_of_domain > 1)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions\n",
    "        print(f\"Error extracting AgeofDomain for {url}: {str(e)}\")\n",
    "        return 0  # Placeholder value or 0 if an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6c5fef1-bd4b-4366-b66f-f667b82a6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dns.resolver\n",
    "\n",
    "def extract_dns_recording(url):\n",
    "    try:\n",
    "        # Extract the domain from the URL\n",
    "        domain = url.split('//')[-1].split('/')[0]\n",
    "\n",
    "        # Query DNS records for the domain\n",
    "        answers = dns.resolver.resolve(domain, 'A')\n",
    "\n",
    "        # Check for abnormalities (e.g., unexpected IP addresses)\n",
    "        abnormality_detected = any(is_abnormal(answer.address) for answer in answers)\n",
    "\n",
    "        # Return 1 if abnormalities are detected, else return 0\n",
    "        return int(abnormality_detected)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions\n",
    "        print(f\"Error extracting DNSRecording for {url}: {str(e)}\")\n",
    "        return 0  # Placeholder value or 0 if an error occurs\n",
    "\n",
    "def is_abnormal(ip_address):\n",
    "    # Placeholder criteria for determining if an IP address is abnormal\n",
    "    # Modify this function based on your specific criteria\n",
    "    # For example, you might check if the IP belongs to known malicious ranges\n",
    "    known_malicious_ranges = ['1.2.3.4', '5.6.7.8']\n",
    "    return ip_address in known_malicious_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d5373c9-c4be-4cb7-b337-281d5665c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_website_traffic(url, api_key):\n",
    "    try:\n",
    "        # Make a request to the SimilarWeb API\n",
    "        endpoint = f\"https://api.similarweb.com/v1/website/{url}/total-traffic-and-engagement\"\n",
    "        params = {\"api_key\": api_key}\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract website traffic information (modify based on the API response structure)\n",
    "        website_traffic = data.get(\"visits\", 0)\n",
    "\n",
    "        return website_traffic\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions\n",
    "        print(f\"Error extracting WebsiteTraffic for {url}: {str(e)}\")\n",
    "        return 0  # Placeholder value or 0 if an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d273e828-e601-4116-9099-fc2bd430648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def extract_alexa_data(url, access_key, secret_key):\n",
    "    try:\n",
    "        # Make a request to the Alexa API\n",
    "        endpoint = \"https://awis.api.alexa.com/api\"\n",
    "        params = {\n",
    "            \"Action\": \"TrafficHistory\",\n",
    "            \"Url\": url,\n",
    "            \"Range\": \"31\",\n",
    "            \"ResponseGroup\": \"History\",\n",
    "        }\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/xml\",\n",
    "            \"x-api-key\": access_key,\n",
    "            \"Authorization\": f\"AWS4-HMAC-SHA256 Credential={access_key}/{params['Timestamp']} Region=us-west-1 Service=execute-api/aws4_request SignedHeaders=content-type;host;x-amz-date;x-api-key Authorization\",\n",
    "        }\n",
    "        response = requests.get(endpoint, params=params, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract website traffic and PageRank information (modify based on the API response structure)\n",
    "        website_traffic = data.get(\"TrafficHistory\", {}).get(\"Data\", [{}])[0].get(\"PageViews\", 0)\n",
    "        pagerank = data.get(\"TrafficHistory\", {}).get(\"Data\", [{}])[0].get(\"Rank\", 0)\n",
    "\n",
    "        return website_traffic, pagerank\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions\n",
    "        print(f\"Error extracting Alexa data for {url}: {str(e)}\")\n",
    "        return 0, 0  # Placeholder values or 0 if an error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "030b67c3-cbce-46df-b247-3ddebfb9e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_links_pointing_to_page(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        inbound_links = soup.find_all('a', href=lambda x: x and url in x)\n",
    "        return len(inbound_links)\n",
    "    except Exception as e:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6b8cfb0-4915-4712-82de-1becabd4d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats_report(url):\n",
    "    return int('stats' in url.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a66b9abe-2714-4e1f-b6d9-5acb87b19c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Form Action: , Method: get\n",
      "Input Type: text\n",
      "Form Action: /search/feedback, Method: post\n",
      "Input Type: hidden\n",
      "Input Type: checkbox\n",
      "Form Action: /search/custom_scopes, Method: post\n",
      "Input Type: hidden\n",
      "Input Type: hidden\n",
      "Input Type: text\n",
      "Input Type: hidden\n",
      "Input Type: text\n",
      "Error analyzing form handling for https://github.com/Akshat-sGit/phishing_url_detection/tree/main: Message: javascript error: Cannot set properties of null (setting 'innerHTML')\n",
      "  (Session info: headless chrome=119.0.6045.159)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102dae004 chromedriver + 4169732\n",
      "1   chromedriver                        0x0000000102da5ff8 chromedriver + 4136952\n",
      "2   chromedriver                        0x00000001029fb500 chromedriver + 292096\n",
      "3   chromedriver                        0x0000000102a00808 chromedriver + 313352\n",
      "4   chromedriver                        0x0000000102a02af4 chromedriver + 322292\n",
      "5   chromedriver                        0x0000000102a7c138 chromedriver + 819512\n",
      "6   chromedriver                        0x0000000102a7b818 chromedriver + 817176\n",
      "7   chromedriver                        0x0000000102a345e8 chromedriver + 525800\n",
      "8   chromedriver                        0x0000000102a354b8 chromedriver + 529592\n",
      "9   chromedriver                        0x0000000102d74334 chromedriver + 3932980\n",
      "10  chromedriver                        0x0000000102d78970 chromedriver + 3950960\n",
      "11  chromedriver                        0x0000000102d5c774 chromedriver + 3835764\n",
      "12  chromedriver                        0x0000000102d79478 chromedriver + 3953784\n",
      "13  chromedriver                        0x0000000102d4eab4 chromedriver + 3779252\n",
      "14  chromedriver                        0x0000000102d95914 chromedriver + 4069652\n",
      "15  chromedriver                        0x0000000102d95a90 chromedriver + 4070032\n",
      "16  chromedriver                        0x0000000102da5c70 chromedriver + 4136048\n",
      "17  libsystem_pthread.dylib             0x0000000183fcd034 _pthread_start + 136\n",
      "18  libsystem_pthread.dylib             0x0000000183fc7e3c thread_start + 8\n",
      "\n",
      "Error extracting AgeofDomain for https://github.com/Akshat-sGit/phishing_url_detection/tree/main: module 'whois' has no attribute 'whois'\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 10, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_features(url):\n",
    "    features = []\n",
    "\n",
    "    # 1. UsingIP\n",
    "    features.append(extract_using_ip(url))\n",
    "\n",
    "    # 2. LongURL\n",
    "    features.append(extract_long_url(url))\n",
    "\n",
    "    # 3. ShortURL\n",
    "    features.append(extract_short_url(url))\n",
    "\n",
    "    # 4. Symbol@\n",
    "    features.append(extract_symbol_at(url))\n",
    "\n",
    "    # 5. Redirecting//\n",
    "    features.append(extract_redirecting(url))\n",
    "\n",
    "    # 6. PrefixSuffix-\n",
    "    features.append(extract_prefix_suffix(url))\n",
    "\n",
    "    # 7. SubDomains\n",
    "    features.append(extract_subdomains(url))\n",
    "\n",
    "    # 8. HTTPS\n",
    "    features.append(extract_https(url))\n",
    "\n",
    "    # 9. DomainRegLen\n",
    "    features.append(extract_domain_reg_len(url))\n",
    "\n",
    "    # 10. Favicon\n",
    "    features.append(has_favicon(url))\n",
    "\n",
    "    # 11. NonStdPort\n",
    "    features.append(extract_non_std_port(url))\n",
    "\n",
    "    # 12. HTTPSDomainURL\n",
    "    features.append(extract_https_domain_url(url))\n",
    "\n",
    "    # 13. RequestURL\n",
    "    features.append(extract_request_url(url))\n",
    "\n",
    "    # 14. AnchorURL\n",
    "    features.append(extract_anchor_url(url))\n",
    "\n",
    "    # 15. LinksInScriptTags\n",
    "    features.append(has_links_in_script_tags(url))\n",
    "\n",
    "    # 16. ServerFormHandler\n",
    "    features.append(extract_server_form_handler(url))\n",
    "\n",
    "    # 17. InfoEmail\n",
    "    features.append(extract_info_email(url))\n",
    "\n",
    "    # 18. AbnormalURL\n",
    "    features.append(extract_abnormal_url(url))\n",
    "\n",
    "    # 19. WebsiteForwarding\n",
    "    features.append(extract_website_forwarding(url))\n",
    "\n",
    "    # 20. StatusBarCust, DisableRightClick, UsingPopupWindow, IframeRedirection\n",
    "    # (These features may require additional logic or external services)\n",
    "\n",
    "    # 21. AgeofDomain\n",
    "    features.append(extract_age_of_domain(url))\n",
    "\n",
    "    # 22. DNSRecording\n",
    "    features.append(extract_dns_recording(url))\n",
    "\n",
    "    # 23. WebsiteTraffic, PageRank, GoogleIndex (Set as 0)\n",
    "    features.extend([0, 0, 0])\n",
    "\n",
    "    # 24. LinksPointingToPage\n",
    "    features.append(count_links_pointing_to_page(url))\n",
    "\n",
    "    # 25. StatsReport\n",
    "    features.append(extract_stats_report(url))\n",
    "\n",
    "    return features\n",
    "\n",
    "# Example usage:\n",
    "url_to_check = input()\n",
    "url_features = extract_features(url_to_check)\n",
    "print(url_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
